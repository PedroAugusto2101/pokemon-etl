# ETL no Databricks com Data Lakehouse â€” Guia PrÃ¡tico

> ğŸ“š Guia completo e didÃ¡tico para construir pipelines ETL no Databricks usando a arquitetura Lakehouse (Delta Lake + Unity Catalog)

[![Databricks](https://img.shields.io/badge/Databricks-FF3621?style=flat&logo=databricks&logoColor=white)](https://databricks.com/)
[![Delta Lake](https://img.shields.io/badge/Delta%20Lake-00ADD8?style=flat&logo=deltalake&logoColor=white)](https://delta.io/)
[![Apache Spark](https://img.shields.io/badge/Apache%20Spark-E25A1C?style=flat&logo=apachespark&logoColor=white)](https://spark.apache.org/)

---

## ğŸ“– SumÃ¡rio

- [ğŸ” VisÃ£o Geral](#-visÃ£o-geral)
- [ğŸ—ï¸ Arquitetura de Alto NÃ­vel](#ï¸-arquitetura-de-alto-nÃ­vel)
- [ğŸ§  Conceitos Fundamentais](#-conceitos-fundamentais)
- [ğŸ’» Workspace e GitOps](#-workspace-e-gitops)
- [âœ… Boas PrÃ¡ticas](#-boas-prÃ¡ticas)
- [ğŸš€ Pipeline ETL - Passo a Passo](#-pipeline-etl---passo-a-passo)
- [âš¡ Performance e Custo](#-performance-e-custo)
- [ğŸ”’ SeguranÃ§a e GovernanÃ§a](#-seguranÃ§a-e-governanÃ§a)
- [ğŸ“Š Monitoramento e Auditoria](#-monitoramento-e-auditoria)
- [ğŸ“ Estrutura do Projeto](#-estrutura-do-projeto)
- [ğŸ”§ Snippets Ãšteis](#-snippets-Ãºteis)
- [ğŸ“š GlossÃ¡rio](#-glossÃ¡rio)

---

## ğŸ” VisÃ£o Geral

### O que Ã© o Databricks?

O **Databricks** Ã© uma **plataforma unificada de dados** que integra:

- ğŸ”§ **Engenharia de Dados**
- ğŸ§ª **CiÃªncia de Dados**
- ğŸ“Š **Analytics**
- ğŸ’¼ **Business Intelligence**

### CaracterÃ­sticas Principais

| Aspecto           | DescriÃ§Ã£o                                                |
| ----------------- | -------------------------------------------------------- |
| **Arquitetura**   | Roda **sobre** clouds (AWS/Azure/GCP), nÃ£o Ã© um provedor |
| **Interface**     | Desenvolvimento via navegador, sem instalaÃ§Ã£o local      |
| **Gerenciamento** | Automatiza clusters, jobs, catÃ¡logos e lineage           |
| **Lakehouse**     | Combina flexibilidade de data lake + recursos de banco   |

> ğŸ’¡ **Lakehouse** = Data Lake + Delta Lake + Unity Catalog (governanÃ§a)

---

## ğŸ—ï¸ Arquitetura de Alto NÃ­vel

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Plataforma Databricks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                     â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚ â”‚   Workspace     â”‚ â”‚   Workflows     â”‚ â”‚ Unity Catalog   â”‚         â”‚
â”‚ â”‚ (Notebooks/.py) â”‚ â”‚    (Jobs)       â”‚ â”‚ (GovernanÃ§a)    â”‚         â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                              â”‚                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚     Compute / Clusters Spark    â”‚
              â”‚      Driver + Workers           â”‚
              â”‚   (Autoscaling, Photon)         â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚      I/O Operations             â”‚
              â”‚   (Read/Write Storage)          â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â–¼                                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Cloud Storage     â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚     Delta Lake      â”‚
â”‚ S3/ADLS/GCS        â”‚                  â”‚ Time Travel/ACID    â”‚
â”‚ (Volumes/Tables)    â”‚                  â”‚    Governance       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§  Conceitos Fundamentais

### ğŸ¢ Databricks vs Cloud Provider

| Aspecto           | Databricks         | Cloud Provider         |
| ----------------- | ------------------ | ---------------------- |
| **FunÃ§Ã£o**        | Orquestra recursos | Fornece infraestrutura |
| **CobranÃ§a**      | Plataforma + Infra | Apenas infraestrutura  |
| **Gerenciamento** | Automatizado       | Manual                 |
| **Compute**       | EfÃªmero (se perde) | Persistente            |

### âš™ï¸ Compute (Clusters)

#### Componentes

- **Driver**: NÃ³ coordenador (1 por cluster)
- **Workers**: NÃ³s de processamento (escalÃ¡veis)

#### Tipos de Runtime

| Runtime      | DescriÃ§Ã£o         | Uso Recomendado           |
| ------------ | ----------------- | ------------------------- |
| **Standard** | Spark padrÃ£o      | Workloads gerais          |
| **LTS**      | Long Term Support | ProduÃ§Ã£o                  |
| **Photon**   | Motor vetorizado  | SQL/DataFrames intensivos |

#### EstratÃ©gias de Custo

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   On-Demand     â”‚    â”‚      Spot       â”‚    â”‚  Auto-Scaling   â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ âœ… DisponÃ­vel   â”‚    â”‚ âœ… Mais barato  â”‚    â”‚ âœ… DinÃ¢mico     â”‚
â”‚ âŒ Mais caro    â”‚    â”‚ âŒ PreemptÃ­vel  â”‚    â”‚ âš¡ Baseado em   â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚    demanda      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ’¾ Storage (Data Lake/Lakehouse)

#### SeparaÃ§Ã£o de Responsabilidades

- **Compute**: Processa dados (efÃªmero)
- **Storage**: Armazena dados (persistente)

#### EvoluÃ§Ã£o: Lake â†’ Lakehouse

```text
Data Lake          +          Database Features          =          Lakehouse
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Flexibilidade    +    â€¢ ACID Transactions           =    â€¢ Melhor de ambos
â€¢ Baixo custo      +    â€¢ Schema enforcement          =    â€¢ Delta Lake
â€¢ Escalabilidade   +    â€¢ Time Travel                 =    â€¢ Unity Catalog
â€¢ Formato aberto   +    â€¢ GovernanÃ§a                  =    â€¢ Performance
```

### ğŸ”º Delta Lake

#### CaracterÃ­sticas Principais

| Recurso                 | BenefÃ­cio             |
| ----------------------- | --------------------- |
| **ACID**                | TransaÃ§Ãµes confiÃ¡veis |
| **Time Travel**         | HistÃ³rico de versÃµes  |
| **Schema Evolution**    | EvoluÃ§Ã£o segura       |
| **MERGE/UPDATE/DELETE** | OperaÃ§Ãµes DML         |
| **OtimizaÃ§Ãµes**         | Z-Order, compactaÃ§Ã£o  |

#### Transaction Log

```text
ğŸ“ tabela_delta/
â”œâ”€â”€ ğŸ“„ part-00000.parquet
â”œâ”€â”€ ğŸ“„ part-00001.parquet
â””â”€â”€ ğŸ“ _delta_log/
    â”œâ”€â”€ ğŸ“„ 00000000000000000000.json  â† TransaÃ§Ã£o 0
    â”œâ”€â”€ ğŸ“„ 00000000000000000001.json  â† TransaÃ§Ã£o 1
    â””â”€â”€ ğŸ“„ 00000000000000000002.json  â† TransaÃ§Ã£o 2
```

### ğŸ›ï¸ Unity Catalog (GovernanÃ§a)

#### Hierarquia

```text
ğŸ¢ Metastore
â”œâ”€â”€ ğŸ“š Catalog A
â”‚   â”œâ”€â”€ ğŸ“‚ Schema 1
â”‚   â”‚   â”œâ”€â”€ ğŸ“‹ Table X
â”‚   â”‚   â””â”€â”€ ğŸ‘ï¸ View Y
â”‚   â””â”€â”€ ğŸ“‚ Schema 2
â””â”€â”€ ğŸ“š Catalog B
    â””â”€â”€ ğŸ“‚ Schema 3
```

#### Managed vs External Tables

| Tipo         | Gerenciamento | Ao Dropar       | Uso Recomendado      |
| ------------ | ------------- | --------------- | -------------------- |
| **Managed**  | Databricks    | Apaga arquivos  | Dados internos       |
| **External** | UsuÃ¡rio       | MantÃ©m arquivos | Dados compartilhados |

### ğŸ“¦ Volumes, Mounts e Buckets

#### EvoluÃ§Ã£o dos PadrÃµes

```text
Legacy (Mounts)          â†’          Modern (Unity Catalog)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
/mnt/bucket-name/        â†’          /Volumes/catalog/schema/volume/

â€¢ Sem governanÃ§a         â†’          â€¢ GovernanÃ§a integrada
â€¢ PermissÃµes complexas   â†’          â€¢ PermissÃµes Unity Catalog
â€¢ Deprecated             â†’          â€¢ Recomendado
```

---

## ğŸ’» Workspace e GitOps

### ğŸ”„ IntegraÃ§Ã£o com Git

#### Workflow Recomendado

```text
1. ğŸŒ¿ Feature Branch     â†’     2. ğŸ’» Desenvolvimento     â†’     3. ğŸ” Code Review
   feat/nova-feature            Notebooks/Scripts               Pull Request
           â”‚                           â”‚                           â”‚
           â–¼                           â–¼                           â–¼
4. âœ… AprovaÃ§Ã£o         â†’     5. ğŸ”„ Merge Main         â†’     6. ğŸš€ Deploy
   Code Review                 Branch Principal              ProduÃ§Ã£o
```

### ğŸ“ Notebooks vs Scripts

| Aspecto               | Notebooks         | Scripts (.py) |
| --------------------- | ----------------- | ------------- |
| **ExploraÃ§Ã£o**        | âœ… Excelente      | âŒ Limitado   |
| **VisualizaÃ§Ã£o**      | âœ… Integrada      | âŒ Externa    |
| **SQL + Python**      | âœ… Magic commands | âŒ Separado   |
| **Reprodutibilidade** | âš ï¸ Moderada       | âœ… Excelente  |
| **Testes**            | âŒ DifÃ­cil        | âœ… FÃ¡cil      |
| **CI/CD**             | âš ï¸ Complexo       | âœ… Simples    |

### ğŸ¯ Dicas de Produtividade

```python
# Magic commands Ãºteis
%sql SELECT * FROM tabela LIMIT 10
%fs ls /Volumes/catalog/schema/volume/
%sh pip install requests

# SQL â†’ DataFrame Python
# Cell SQL
SELECT * FROM bronze.projeto.tabela;

# Cell Python (prÃ³xima)
df = _sqldf  # Ãšltimo resultado SQL
```

---

## âœ… Boas PrÃ¡ticas

### ğŸ—ï¸ Arquitetura Medallion

```text
ğŸ—‚ï¸ Raw/Landing     â†’     ğŸ¥‰ Bronze     â†’     ğŸ¥ˆ Silver     â†’     ğŸ¥‡ Gold
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ“ Arquivos         â†’    ğŸ“‹ Tabelas    â†’    ğŸ“‹ Tabelas    â†’    ğŸ“‹ Tabelas
   Brutos                 MÃ­nimas           Limpas            AnalÃ­ticas

â€¢ Como chegou       â†’    â€¢ DeduplicaÃ§Ã£o â†’   â€¢ Joins        â†’   â€¢ AgregaÃ§Ãµes
â€¢ JSON/CSV/Parquet  â†’    â€¢ Timestamps   â†’   â€¢ ValidaÃ§Ãµes   â†’   â€¢ KPIs
â€¢ Volumes           â†’    â€¢ Schema basic â†’   â€¢ NormalizaÃ§Ã£o â†’   â€¢ Dashboards
```

### ğŸ”„ PrincÃ­pios de Design

#### 1. Desacoplamento

```text
âŒ ExtraÃ§Ã£o + TransformaÃ§Ã£o Acoplada
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ API â†’ Transform â†’ Save              â”‚  â† Se API muda, quebra tudo
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

âœ… ExtraÃ§Ã£o Desacoplada
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ API â†’   â”‚ â†’  â”‚ Raw â†’   â”‚ â†’  â”‚ Bronze  â”‚
â”‚ Save    â”‚    â”‚ Bronze  â”‚    â”‚ Silver  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 2. IdempotÃªncia

```sql
-- âŒ NÃ£o idempotente
INSERT INTO tabela SELECT * FROM fonte;

-- âœ… Idempotente
MERGE INTO tabela AS t
USING fonte AS s ON t.id = s.id
WHEN MATCHED THEN UPDATE SET *
WHEN NOT MATCHED THEN INSERT *;
```

### ğŸ“ ConvenÃ§Ãµes de Nomenclatura

| Elemento        | PadrÃ£o                | Exemplo              |
| --------------- | --------------------- | -------------------- |
| **Catalog**     | `ambiente`            | `dev`, `prod`        |
| **Schema**      | `dominio_projeto`     | `vendas_ecommerce`   |
| **Tabela**      | `tb_dominio_entidade` | `tb_vendas_pedidos`  |
| **Volume**      | `v_tipo`              | `v_raw`, `v_staging` |
| **Arquivo Raw** | `entidade_YYYY-MM-DD` | `pedidos_2024-01-15` |

### ğŸ”’ SeguranÃ§a

```python
# âœ… Use Databricks Secrets
token = dbutils.secrets.get("key-vault", "api-token")

# âŒ Nunca hardcode credentials
token = "abc123..."  # NUNCA FAÃ‡A ISSO!
```

---

## ğŸš€ Pipeline ETL - Passo a Passo

### ğŸ“‹ PrÃ©-requisitos

#### Checklist de Ambiente

- [ ] âœ… Metastore Unity Catalog criado
- [ ] âœ… Storage Credentials configurado
- [ ] âœ… External Locations criadas
- [ ] âœ… Cluster com Runtime LTS
- [ ] âœ… Photon habilitado (se aplicÃ¡vel)
- [ ] âœ… Auto Termination configurado

### 1ï¸âƒ£ Preparar Ambiente

#### 1.1 Configurar Unity Catalog

```sql
-- Verificar metastore
SHOW METASTORES;

-- Verificar external locations
SHOW EXTERNAL LOCATIONS;
```

#### 1.2 Configurar Cluster

| ConfiguraÃ§Ã£o         | Valor Recomendado       |
| -------------------- | ----------------------- |
| **Runtime**          | LTS (Latest)            |
| **Photon**           | Enabled (SQL workloads) |
| **Auto Termination** | 60 minutos              |
| **Workers**          | 2-8 (com autoscaling)   |

### 2ï¸âƒ£ Criar CatÃ¡logos e Schemas

```sql
-- Estrutura Medallion
CREATE CATALOG IF NOT EXISTS raw;
CREATE CATALOG IF NOT EXISTS bronze;
CREATE CATALOG IF NOT EXISTS silver;
CREATE CATALOG IF NOT EXISTS gold;

-- Schemas por projeto
CREATE SCHEMA IF NOT EXISTS raw.meu_projeto;
CREATE SCHEMA IF NOT EXISTS bronze.meu_projeto;
CREATE SCHEMA IF NOT EXISTS silver.meu_projeto;
CREATE SCHEMA IF NOT EXISTS gold.meu_projeto;
```

### 3ï¸âƒ£ Criar Volumes para Raw

```sql
-- Volume para arquivos brutos
CREATE VOLUME IF NOT EXISTS raw.meu_projeto.v_raw
LOCATION 's3://meu-bucket/landing/meu_projeto/';

-- Verificar criaÃ§Ã£o
SHOW VOLUMES IN raw.meu_projeto;
```

### 4ï¸âƒ£ IngestÃ£o Raw (Arquivos Brutos)

#### Script de IngestÃ£o API

```python
# Notebook: ingestion/01_raw_api_ingestion
import json
import datetime as dt
import requests

# ConfiguraÃ§Ãµes
API_URL = "https://pokeapi.co/api/v2/pokemon?limit=1000"
run_ts = dt.datetime.utcnow().strftime("%Y-%m-%d_%H%M%S")
base_dir = f"/Volumes/raw/meu_projeto/v_raw/pokemon/extract_ts={run_ts}"

# Criar diretÃ³rio
dbutils.fs.mkdirs(base_dir)

# Extrair dados
try:
    response = requests.get(API_URL, timeout=30)
    response.raise_for_status()

    # Salvar como chegou
    file_path = f"{base_dir}/pokemon.json"
    dbutils.fs.put(file_path, json.dumps(response.json()), overwrite=True)

    print(f"âœ… Dados salvos em: {file_path}")

except Exception as e:
    print(f"âŒ Erro na ingestÃ£o: {e}")
    raise
```

### 5ï¸âƒ£ Tabelas Bronze

#### Processamento Bronze

```python
# Notebook: transform/02_bronze_processing
from pyspark.sql.functions import (
    input_file_name,
    current_timestamp,
    regexp_extract
)

# Ler dados raw
raw_path = "/Volumes/raw/meu_projeto/v_raw/pokemon/*/*.json"
df_raw = spark.read.option("multiline", "true").json(raw_path)

# Adicionar metadados tÃ©cnicos
df_bronze = (
    df_raw
    .dropDuplicates()
    .withColumn("_ingestion_ts", current_timestamp())
    .withColumn("_source_file", input_file_name())
    .withColumn("_extract_date",
                regexp_extract(input_file_name(), r"extract_ts=(\d{4}-\d{2}-\d{2})", 1))
)

# Salvar como tabela Delta
(df_bronze
 .write
 .format("delta")
 .mode("overwrite")
 .option("mergeSchema", "true")
 .saveAsTable("bronze.meu_projeto.tb_pokemon_raw"))

print("âœ… Tabela Bronze criada com sucesso!")
```

### 6ï¸âƒ£ Tabelas Silver

#### Limpeza e NormalizaÃ§Ã£o

```sql
-- Notebook: transform/03_silver_processing

-- Criar tabela Silver inicial
CREATE OR REPLACE TABLE silver.meu_projeto.tb_pokemon_clean
USING DELTA
AS
SELECT
  -- Dados de negÃ³cio
  explode(results) as pokemon_data,
  _ingestion_ts,
  _extract_date
FROM bronze.meu_projeto.tb_pokemon_raw;

-- Normalizar estrutura
CREATE OR REPLACE TABLE silver.meu_projeto.tb_pokemon_list
USING DELTA
AS
SELECT
  pokemon_data.name as pokemon_name,
  pokemon_data.url as pokemon_url,
  regexp_extract(pokemon_data.url, r'/(\d+)/', 1)::int as pokemon_id,
  _ingestion_ts,
  _extract_date,
  current_timestamp() as _silver_processed_ts
FROM silver.meu_projeto.tb_pokemon_clean
WHERE pokemon_data.name IS NOT NULL;
```

#### Upsert Pattern

```sql
-- Merge incremental (idempotente)
MERGE INTO silver.meu_projeto.tb_pokemon_list AS target
USING (
  SELECT DISTINCT
    pokemon_name,
    pokemon_url,
    pokemon_id,
    _ingestion_ts,
    _extract_date,
    current_timestamp() as _silver_processed_ts
  FROM silver.meu_projeto.tb_pokemon_clean
  WHERE _extract_date = current_date()
) AS source
ON target.pokemon_id = source.pokemon_id
WHEN MATCHED THEN
  UPDATE SET *
WHEN NOT MATCHED THEN
  INSERT *;
```

### 7ï¸âƒ£ Tabelas Gold

#### Modelos AnalÃ­ticos

```sql
-- Notebook: transform/04_gold_analytics

-- MÃ©tricas de negÃ³cio
CREATE OR REPLACE TABLE gold.meu_projeto.vw_pokemon_metrics
USING DELTA
AS
SELECT
  COUNT(*) as total_pokemon,
  COUNT(DISTINCT _extract_date) as total_extractions,
  MIN(_ingestion_ts) as first_ingestion,
  MAX(_ingestion_ts) as last_ingestion,
  current_timestamp() as _gold_processed_ts
FROM silver.meu_projeto.tb_pokemon_list;

-- AnÃ¡lise temporal
CREATE OR REPLACE TABLE gold.meu_projeto.vw_pokemon_daily_stats
USING DELTA
AS
SELECT
  _extract_date,
  COUNT(*) as pokemon_count,
  COUNT(DISTINCT pokemon_name) as unique_pokemon,
  MIN(_ingestion_ts) as batch_start,
  MAX(_ingestion_ts) as batch_end
FROM silver.meu_projeto.tb_pokemon_list
GROUP BY _extract_date
ORDER BY _extract_date DESC;
```

### 8ï¸âƒ£ OrquestraÃ§Ã£o com Workflows

#### ConfiguraÃ§Ã£o do Job

```json
{
  "name": "pokemon_etl_pipeline",
  "description": "Pipeline ETL completo para dados Pokemon",
  "git_source": {
    "git_provider": "github",
    "git_url": "https://github.com/usuario/projeto",
    "git_branch": "main"
  },
  "tasks": [
    {
      "task_key": "raw_ingestion",
      "description": "IngestÃ£o de dados brutos da API",
      "notebook_task": {
        "notebook_path": "src/ingestion/01_raw_api_ingestion"
      },
      "job_cluster_key": "etl_cluster"
    },
    {
      "task_key": "bronze_processing",
      "description": "Processamento Bronze",
      "depends_on": [{ "task_key": "raw_ingestion" }],
      "notebook_task": {
        "notebook_path": "src/transform/02_bronze_processing"
      },
      "job_cluster_key": "etl_cluster"
    },
    {
      "task_key": "silver_processing",
      "description": "Processamento Silver",
      "depends_on": [{ "task_key": "bronze_processing" }],
      "sql_task": {
        "file": {
          "path": "src/transform/03_silver_processing.sql"
        }
      },
      "job_cluster_key": "etl_cluster"
    },
    {
      "task_key": "gold_analytics",
      "description": "CriaÃ§Ã£o de modelos Gold",
      "depends_on": [{ "task_key": "silver_processing" }],
      "sql_task": {
        "file": {
          "path": "src/transform/04_gold_analytics.sql"
        }
      },
      "job_cluster_key": "etl_cluster"
    }
  ],
  "job_clusters": [
    {
      "job_cluster_key": "etl_cluster",
      "new_cluster": {
        "cluster_name": "",
        "spark_version": "13.3.x-scala2.12",
        "node_type_id": "i3.xlarge",
        "num_workers": 2,
        "autoscale": {
          "min_workers": 1,
          "max_workers": 4
        }
      }
    }
  ],
  "schedule": {
    "quartz_cron_expression": "0 0 6 * * ?",
    "timezone_id": "UTC"
  }
}
```

### 9ï¸âƒ£ ParametrizaÃ§Ã£o

#### Notebooks com Widgets

```python
# Criar parÃ¢metros
dbutils.widgets.text("process_date", "")
dbutils.widgets.dropdown("environment", "dev", ["dev", "staging", "prod"])

# Usar parÃ¢metros
process_date = dbutils.widgets.get("process_date")
environment = dbutils.widgets.get("environment")

# ValidaÃ§Ã£o
if not process_date:
    process_date = datetime.now().strftime("%Y-%m-%d")

print(f"Processando data: {process_date} no ambiente: {environment}")
```

#### Scripts Python com ArgumentParser

```python
# src/utils/config.py
import argparse
from datetime import datetime

def get_config():
    parser = argparse.ArgumentParser(description='ETL Pokemon Pipeline')
    parser.add_argument('--process-date',
                       default=datetime.now().strftime("%Y-%m-%d"),
                       help='Data de processamento (YYYY-MM-DD)')
    parser.add_argument('--environment',
                       choices=['dev', 'staging', 'prod'],
                       default='dev',
                       help='Ambiente de execuÃ§Ã£o')
    return parser.parse_args()

# Uso no script
if __name__ == "__main__":
    config = get_config()
    print(f"Executando para {config.process_date} em {config.environment}")
```

### ğŸ”Ÿ IngestÃ£o Incremental (Streaming)

#### Auto Loader Pattern

```python
# ConfiguraÃ§Ã£o do stream
checkpoint_path = "/Volumes/raw/meu_projeto/v_raw/_checkpoints/pokemon_bronze"
source_path = "/Volumes/raw/meu_projeto/v_raw/pokemon"

# Stream de leitura com Auto Loader
stream_df = (spark.readStream
             .format("cloudFiles")
             .option("cloudFiles.format", "json")
             .option("cloudFiles.schemaLocation", f"{checkpoint_path}/schema")
             .option("cloudFiles.inferColumnTypes", "true")
             .load(source_path))

# Adicionar metadados de streaming
stream_processed = (stream_df
                   .withColumn("_stream_ingestion_ts", current_timestamp())
                   .withColumn("_source_file", input_file_name()))

# Escrever como stream
query = (stream_processed.writeStream
         .format("delta")
         .outputMode("append")
         .option("checkpointLocation", checkpoint_path)
         .trigger(availableNow=True)  # Batch incremental
         .toTable("bronze.meu_projeto.tb_pokemon_stream"))

# Aguardar conclusÃ£o
query.awaitTermination()
print("âœ… Stream processado com sucesso!")
```

### 1ï¸âƒ£1ï¸âƒ£ ExposiÃ§Ã£o para BI

#### SQL Warehouse Setup

```sql
-- Criar view otimizada para BI
CREATE OR REPLACE VIEW gold.meu_projeto.pokemon_dashboard AS
SELECT
  pokemon_name,
  pokemon_id,
  _extract_date as data_coleta,
  CASE
    WHEN pokemon_id <= 151 THEN 'GeraÃ§Ã£o 1'
    WHEN pokemon_id <= 251 THEN 'GeraÃ§Ã£o 2'
    ELSE 'Outras'
  END as geracao,
  _silver_processed_ts as ultima_atualizacao
FROM silver.meu_projeto.tb_pokemon_list
ORDER BY pokemon_id;

-- Grant de acesso para grupo BI
GRANT SELECT ON gold.meu_projeto.pokemon_dashboard TO `bi-team`;
```

#### Power BI Connection

```text
ğŸ“‹ Checklist Power BI:
1. âœ… Criar SQL Warehouse no Databricks
2. âœ… Copiar Server Hostname e HTTP Path
3. âœ… Instalar Databricks Connector no Power BI
4. âœ… Configurar autenticaÃ§Ã£o (Token/OAuth)
5. âœ… Testar conexÃ£o
6. âœ… Criar relatÃ³rios
```

---

## âš¡ Performance e Custo

### ğŸš€ OtimizaÃ§Ãµes de Performance

#### Delta Lake Optimizations

```sql
-- CompactaÃ§Ã£o e Z-Ordering
OPTIMIZE silver.meu_projeto.tb_pokemon_list
ZORDER BY (pokemon_id, _extract_date);

-- Auto Optimize (configuraÃ§Ã£o de tabela)
ALTER TABLE silver.meu_projeto.tb_pokemon_list
SET TBLPROPERTIES (
  'delta.autoOptimize.optimizeWrite' = 'true',
  'delta.autoOptimize.autoCompact' = 'true'
);

-- Particionamento por data
CREATE TABLE silver.meu_projeto.tb_pokemon_partitioned
USING DELTA
PARTITIONED BY (_extract_date)
AS SELECT * FROM silver.meu_projeto.tb_pokemon_list;
```

#### Cluster Optimizations

| ConfiguraÃ§Ã£o         | Impacto   | RecomendaÃ§Ã£o            |
| -------------------- | --------- | ----------------------- |
| **Photon**           | ğŸš€ High   | SQL/DataFrame workloads |
| **Autoscaling**      | ğŸ’° Medium | Min=1, Max=8 workers    |
| **Spot Instances**   | ğŸ’° High   | Batch tolerante         |
| **Auto Termination** | ğŸ’° High   | 30-60 minutos           |

### ğŸ’° EstratÃ©gias de Custo

#### Compute Cost Optimization

```text
ğŸ’¡ Dicas de Economia:

1. ğŸ¯ Right-sizing
   â”œâ”€â”€ Use clusters menores para desenvolvimento
   â”œâ”€â”€ Autoscaling baseado em demanda
   â””â”€â”€ Job clusters vs All-purpose clusters

2. â° Scheduling
   â”œâ”€â”€ Jobs em horÃ¡rios de menor demanda
   â”œâ”€â”€ Auto-termination agressivo
   â””â”€â”€ Pause clusters nÃ£o utilizados

3. ğŸ’¾ Storage
   â”œâ”€â”€ Lifecycle policies (S3/ADLS)
   â”œâ”€â”€ VACUUM regulares
   â””â”€â”€ CompactaÃ§Ã£o automÃ¡tica
```

#### Monitoring de Custos

```python
# Verificar mÃ©tricas de cluster
cluster_metrics = spark.sql("""
  SELECT
    cluster_id,
    cluster_name,
    uptime_hours,
    cost_estimate
  FROM system.compute.clusters
  WHERE cluster_name LIKE '%meu_projeto%'
""")

display(cluster_metrics)
```

---

## ğŸ”’ SeguranÃ§a e GovernanÃ§a

### ğŸ›¡ï¸ Unity Catalog Security

#### Modelo de PermissÃµes

```sql
-- Criar grupos
CREATE GROUP data_engineers;
CREATE GROUP data_analysts;
CREATE GROUP bi_users;

-- PermissÃµes por camada
-- Bronze: Apenas Data Engineers
GRANT SELECT, INSERT, UPDATE, DELETE ON CATALOG bronze TO data_engineers;

-- Silver: Data Engineers + Data Analysts
GRANT SELECT, INSERT, UPDATE, DELETE ON CATALOG silver TO data_engineers;
GRANT SELECT ON CATALOG silver TO data_analysts;

-- Gold: Todos os grupos
GRANT SELECT, INSERT, UPDATE, DELETE ON CATALOG gold TO data_engineers;
GRANT SELECT ON CATALOG gold TO data_analysts;
GRANT SELECT ON CATALOG gold TO bi_users;
```

#### Row-Level Security

```sql
-- View com filtro dinÃ¢mico
CREATE VIEW gold.meu_projeto.pokemon_filtered AS
SELECT *
FROM gold.meu_projeto.vw_pokemon_daily_stats
WHERE CASE
  WHEN is_member('admin') THEN TRUE
  WHEN is_member('data_analysts') THEN _extract_date >= current_date() - 30
  ELSE _extract_date >= current_date() - 7
END;
```

### ğŸ” Secrets Management

```python
# Configurar secrets scope
# CLI: databricks secrets create-scope --scope "api-keys"

# Usar secrets no cÃ³digo
def get_api_credentials():
    return {
        'api_key': dbutils.secrets.get("api-keys", "pokemon-api-key"),
        'base_url': dbutils.secrets.get("api-keys", "pokemon-base-url")
    }

# âŒ NUNCA faÃ§a isso:
# api_key = "hardcoded-key-123"  # VULNERABILIDADE!
```

### ğŸ“Š Data Quality

#### ValidaÃ§Ãµes AutomÃ¡ticas

```python
# src/utils/data_quality.py
from pyspark.sql.functions import col, count, when, isnan, isnull

def run_data_quality_checks(df, table_name):
    """Executa verificaÃ§Ãµes de qualidade de dados"""

    total_rows = df.count()

    # VerificaÃ§Ãµes bÃ¡sicas
    quality_checks = {}

    for column in df.columns:
        null_count = df.filter(col(column).isNull()).count()
        null_percentage = (null_count / total_rows) * 100

        quality_checks[column] = {
            'null_count': null_count,
            'null_percentage': round(null_percentage, 2)
        }

    # Log resultados
    print(f"ğŸ“Š Quality Check para {table_name}:")
    print(f"   Total de registros: {total_rows}")

    for col_name, metrics in quality_checks.items():
        if metrics['null_percentage'] > 10:  # Threshold
            print(f"   âš ï¸  {col_name}: {metrics['null_percentage']}% nulls")
        else:
            print(f"   âœ… {col_name}: {metrics['null_percentage']}% nulls")

    return quality_checks

# Uso
quality_results = run_data_quality_checks(df_silver, "tb_pokemon_list")
```

---

## ğŸ“Š Monitoramento e Auditoria

### ğŸ“ˆ Data Lineage

#### VisualizaÃ§Ã£o AutomÃ¡tica

```sql
-- Verificar lineage via SQL
SHOW LINEAGE TABLE silver.meu_projeto.tb_pokemon_list;

-- HistÃ³rico de alteraÃ§Ãµes
DESCRIBE HISTORY silver.meu_projeto.tb_pokemon_list;
```

#### Lineage ProgramÃ¡tico

```python
# Capturar informaÃ§Ãµes de lineage
def log_lineage(source_table, target_table, transformation_type):
    lineage_info = {
        'source': source_table,
        'target': target_table,
        'transformation': transformation_type,
        'timestamp': datetime.now(),
        'user': spark.sql("SELECT current_user()").collect()[0][0],
        'notebook': dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()
    }

    # Log em tabela de auditoria
    lineage_df = spark.createDataFrame([lineage_info])
    (lineage_df.write
     .format("delta")
     .mode("append")
     .saveAsTable("system.audit.lineage_log"))

# Uso
log_lineage("bronze.projeto.tb_raw", "silver.projeto.tb_clean", "cleaning")
```

### ğŸ” Time Travel e Auditoria

#### Consultas HistÃ³ricas

```sql
-- Ver todas as versÃµes
DESCRIBE HISTORY silver.meu_projeto.tb_pokemon_list;

-- Consultar versÃ£o especÃ­fica
SELECT COUNT(*) as registros_v0
FROM silver.meu_projeto.tb_pokemon_list VERSION AS OF 0;

-- Consultar por timestamp
SELECT *
FROM silver.meu_projeto.tb_pokemon_list
TIMESTAMP AS OF '2024-01-15T10:00:00Z'
LIMIT 10;

-- Comparar versÃµes
WITH v0 AS (
  SELECT COUNT(*) as count_v0
  FROM silver.meu_projeto.tb_pokemon_list VERSION AS OF 0
),
current AS (
  SELECT COUNT(*) as count_current
  FROM silver.meu_projeto.tb_pokemon_list
)
SELECT
  count_v0,
  count_current,
  count_current - count_v0 as difference
FROM v0 CROSS JOIN current;
```

#### Restore e Rollback

```sql
-- Restaurar versÃ£o anterior
RESTORE silver.meu_projeto.tb_pokemon_list VERSION AS OF 2;

-- Ou por timestamp
RESTORE silver.meu_projeto.tb_pokemon_list
TIMESTAMP AS OF '2024-01-15T09:00:00Z';
```

### ğŸ“‹ Monitoring Dashboard

#### MÃ©tricas de Pipeline

```sql
-- View de monitoramento
CREATE OR REPLACE VIEW system.monitoring.pipeline_health AS
SELECT
  'pokemon_pipeline' as pipeline_name,
  -- MÃ©tricas Bronze
  (SELECT COUNT(*) FROM bronze.meu_projeto.tb_pokemon_raw) as bronze_count,
  (SELECT MAX(_ingestion_ts) FROM bronze.meu_projeto.tb_pokemon_raw) as bronze_last_update,

  -- MÃ©tricas Silver
  (SELECT COUNT(*) FROM silver.meu_projeto.tb_pokemon_list) as silver_count,
  (SELECT MAX(_silver_processed_ts) FROM silver.meu_projeto.tb_pokemon_list) as silver_last_update,

  -- MÃ©tricas Gold
  (SELECT COUNT(*) FROM gold.meu_projeto.vw_pokemon_daily_stats) as gold_count,

  -- Health check
  CASE
    WHEN (SELECT MAX(_ingestion_ts) FROM bronze.meu_projeto.tb_pokemon_raw) > current_timestamp() - INTERVAL 1 DAY
    THEN 'Healthy'
    ELSE 'Stale'
  END as pipeline_status,

  current_timestamp() as check_timestamp;
```

#### Alertas AutomÃ¡ticos

```python
# src/monitoring/alerts.py
def check_pipeline_health():
    """Verifica saÃºde do pipeline e envia alertas"""

    health_df = spark.sql("SELECT * FROM system.monitoring.pipeline_health")
    health_data = health_df.collect()[0]

    # VerificaÃ§Ãµes
    alerts = []

    # Data freshness
    if health_data.bronze_last_update < datetime.now() - timedelta(hours=25):
        alerts.append({
            'type': 'DATA_FRESHNESS',
            'message': f'Bronze data stale: {health_data.bronze_last_update}',
            'severity': 'HIGH'
        })

    # Data quality
    bronze_count = health_data.bronze_count
    silver_count = health_data.silver_count

    if silver_count < bronze_count * 0.95:  # 5% tolerance
        alerts.append({
            'type': 'DATA_QUALITY',
            'message': f'Significant data loss: Bronze({bronze_count}) vs Silver({silver_count})',
            'severity': 'MEDIUM'
        })

    # Enviar alertas (implementar webhook/email)
    if alerts:
        send_alerts(alerts)

    return alerts

def send_alerts(alerts):
    """Enviar alertas via webhook/email/Slack"""
    # Implementar integraÃ§Ã£o
    for alert in alerts:
        print(f"ğŸš¨ ALERT [{alert['severity']}]: {alert['message']}")
```

---

## ğŸ“ Estrutura do Projeto

### ğŸ—‚ï¸ Layout Recomendado

```text
ğŸ“¦ databricks-pokemon-etl/
â”œâ”€â”€ ğŸ“‹ README.md
â”œâ”€â”€ ğŸ“‹ requirements.txt
â”œâ”€â”€ ğŸ“‹ setup.py
â”œâ”€â”€ ğŸ“‹ .gitignore
â”œâ”€â”€ ğŸ“‹ .databricks-cli
â”‚
â”œâ”€â”€ ğŸ“ src/
â”‚   â”œâ”€â”€ ğŸ“ ingestion/
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ 01_raw_api_ingestion.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ 02_raw_file_ingestion.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ transform/
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ 02_bronze_processing.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ 03_silver_processing.sql
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ 04_gold_analytics.sql
â”‚   â”‚   â””â”€â”€ ğŸ“„ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ utils/
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ config.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ data_quality.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ io_helpers.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ __init__.py
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“ monitoring/
â”‚       â”œâ”€â”€ ğŸ“„ alerts.py
â”‚       â”œâ”€â”€ ğŸ“„ metrics.py
â”‚       â””â”€â”€ ğŸ“„ __init__.py
â”‚
â”œâ”€â”€ ğŸ“ notebooks/
â”‚   â”œâ”€â”€ ğŸ“ exploration/
â”‚   â”‚   â”œâ”€â”€ ğŸ““ 01_data_exploration.py
â”‚   â”‚   â”œâ”€â”€ ğŸ““ 02_schema_analysis.sql
â”‚   â”‚   â””â”€â”€ ğŸ““ 03_quality_checks.py
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ development/
â”‚   â”‚   â”œâ”€â”€ ğŸ““ prototype_bronze.py
â”‚   â”‚   â”œâ”€â”€ ğŸ““ prototype_silver.py
â”‚   â”‚   â””â”€â”€ ğŸ““ prototype_gold.py
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“ dashboards/
â”‚       â”œâ”€â”€ ğŸ““ pokemon_metrics.py
â”‚       â””â”€â”€ ğŸ““ pipeline_monitoring.py
â”‚
â”œâ”€â”€ ğŸ“ workflows/
â”‚   â”œâ”€â”€ ğŸ“„ pokemon_etl_dev.json
â”‚   â”œâ”€â”€ ğŸ“„ pokemon_etl_prod.json
â”‚   â””â”€â”€ ğŸ“„ monitoring_job.json
â”‚
â”œâ”€â”€ ğŸ“ sql/
â”‚   â”œâ”€â”€ ğŸ“„ setup_catalogs.sql
â”‚   â”œâ”€â”€ ğŸ“„ setup_permissions.sql
â”‚   â””â”€â”€ ğŸ“„ setup_monitoring.sql
â”‚
â”œâ”€â”€ ğŸ“ tests/
â”‚   â”œâ”€â”€ ğŸ“ unit/
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ test_data_quality.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ test_transformations.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ integration/
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ test_pipeline_e2e.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ __init__.py
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“„ conftest.py
â”‚
â”œâ”€â”€ ğŸ“ config/
â”‚   â”œâ”€â”€ ğŸ“„ dev.yaml
â”‚   â”œâ”€â”€ ğŸ“„ staging.yaml
â”‚   â”œâ”€â”€ ğŸ“„ prod.yaml
â”‚   â””â”€â”€ ğŸ“„ secrets.yaml.template
â”‚
â””â”€â”€ ğŸ“ docs/
    â”œâ”€â”€ ğŸ“„ setup.md
    â”œâ”€â”€ ğŸ“„ deployment.md
    â”œâ”€â”€ ğŸ“„ troubleshooting.md
    â””â”€â”€ ğŸ“ images/
        â””â”€â”€ ğŸ–¼ï¸ architecture.png
```

### âš™ï¸ ConfiguraÃ§Ã£o de Ambiente

#### requirements.txt

```text
# Databricks runtime jÃ¡ inclui muitas libs
# Adicione apenas o que Ã© especÃ­fico do projeto

requests>=2.31.0
pyyaml>=6.0
pytest>=7.0.0
databricks-cli>=0.17.0

# Para desenvolvimento local (opcional)
pyspark>=3.4.0
delta-spark>=2.4.0
```

#### config/dev.yaml

```yaml
# ConfiguraÃ§Ã£o para ambiente de desenvolvimento
environment: dev

catalogs:
  raw: raw_dev
  bronze: bronze_dev
  silver: silver_dev
  gold: gold_dev

storage:
  base_path: 's3://mybucket/dev/'
  checkpoints: 's3://mybucket/dev/_checkpoints/'

cluster:
  runtime: '13.3.x-scala2.12'
  node_type: 'i3.xlarge'
  min_workers: 1
  max_workers: 2
  photon: true
  auto_termination: 30

api:
  pokemon_base_url: 'https://pokeapi.co/api/v2'
  timeout: 30

monitoring:
  alerts_enabled: false
  webhook_url: null
```

---

## ğŸ”§ Snippets Ãšteis

### ğŸ’¾ OperaÃ§Ãµes Delta Lake

#### Escrita e Leitura

```python
# Escrever DataFrame como tabela Delta
(df.write
 .format("delta")
 .mode("overwrite")  # append, overwrite, ignore, error
 .option("mergeSchema", "true")  # EvoluÃ§Ã£o automÃ¡tica de schema
 .option("overwriteSchema", "true")  # Para mode overwrite
 .saveAsTable("catalog.schema.table"))

# Ler tabela Delta
df = spark.table("catalog.schema.table")

# Ler com filtros
df = spark.table("catalog.schema.table").filter("date >= '2024-01-01'")
```

#### MERGE (Upsert)

```sql
-- Template MERGE genÃ©rico
MERGE INTO {target_table} AS target
USING (
  SELECT DISTINCT *
  FROM {source_table}
  WHERE {incremental_filter}
) AS source
ON {join_condition}
WHEN MATCHED THEN
  UPDATE SET *
WHEN NOT MATCHED THEN
  INSERT *;
```

#### OtimizaÃ§Ãµes

```sql
-- CompactaÃ§Ã£o com Z-Order
OPTIMIZE catalog.schema.table
ZORDER BY (column1, column2);

-- Vacuum (limpeza de arquivos antigos)
VACUUM catalog.schema.table RETAIN 168 HOURS;

-- EstatÃ­sticas (opcional, para cost-based optimizer)
ANALYZE TABLE catalog.schema.table COMPUTE STATISTICS FOR ALL COLUMNS;
```

### ğŸ” Debugging e Monitoramento

#### InformaÃ§Ãµes de Tabela

```sql
-- InformaÃ§Ãµes gerais
DESCRIBE EXTENDED catalog.schema.table;

-- HistÃ³rico de operaÃ§Ãµes
DESCRIBE HISTORY catalog.schema.table;

-- Detalhes de arquivos
DESCRIBE DETAIL catalog.schema.table;

-- Schema evolution
SHOW COLUMNS IN catalog.schema.table;
```

#### Performance Analysis

```python
# AnÃ¡lise de partiÃ§Ãµes
spark.sql("""
  SELECT
    input_file_name() as file_path,
    COUNT(*) as row_count,
    MAX(_ingestion_ts) as max_timestamp
  FROM catalog.schema.table
  GROUP BY input_file_name()
  ORDER BY row_count DESC
""").display()

# EstatÃ­sticas de execuÃ§Ã£o
query = spark.sql("SELECT COUNT(*) FROM large_table")
query.explain(True)  # Plano de execuÃ§Ã£o detalhado
```

### ğŸ“Š UtilitÃ¡rios de Databricks

#### dbutils Essentials

```python
# Sistema de arquivos
dbutils.fs.ls("/Volumes/catalog/schema/volume/")
dbutils.fs.cp("source_path", "dest_path", recurse=True)
dbutils.fs.rm("/path/to/delete", recurse=True)

# InformaÃ§Ãµes do cluster/notebook
dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()
spark.sql("SELECT current_user() as user, current_database() as db").display()

# Secrets (sempre via secrets)
api_key = dbutils.secrets.get("scope-name", "key-name")

# Widgets para parametrizaÃ§Ã£o
dbutils.widgets.text("param_name", "default_value", "Parameter Description")
param_value = dbutils.widgets.get("param_name")
```

#### SQL Magic Commands

```python
# SQL em cÃ©lula Python
result_df = spark.sql("""
  SELECT COUNT(*) as total
  FROM catalog.schema.table
""")

# Magic command (notebook)
# %%sql
# SELECT * FROM catalog.schema.table LIMIT 10;

# Capturar resultado do SQL magic
# df = _sqldf  # Resultado da Ãºltima cÃ©lula %%sql
```

### ğŸŒŠ Streaming Patterns

#### Auto Loader Template

```python
def create_auto_loader_stream(source_path, target_table, checkpoint_path, file_format="json"):
    """Template para Auto Loader stream"""

    stream_df = (spark.readStream
                 .format("cloudFiles")
                 .option("cloudFiles.format", file_format)
                 .option("cloudFiles.schemaLocation", f"{checkpoint_path}/schema")
                 .option("cloudFiles.inferColumnTypes", "true")
                 .option("cloudFiles.maxFilesPerTrigger", 1000)
                 .load(source_path))

    # Adicionar metadados
    stream_processed = (stream_df
                       .withColumn("_stream_ts", current_timestamp())
                       .withColumn("_source_file", input_file_name()))

    # Configurar escrita
    query = (stream_processed.writeStream
             .format("delta")
             .outputMode("append")
             .option("checkpointLocation", checkpoint_path)
             .trigger(availableNow=True)  # Batch mode
             .toTable(target_table))

    return query

# Uso
query = create_auto_loader_stream(
    source_path="/Volumes/raw/project/landing/",
    target_table="bronze.project.streaming_table",
    checkpoint_path="/Volumes/raw/project/_checkpoints/streaming"
)
query.awaitTermination()
```

### ğŸ”— Conectividade e APIs

#### HTTP Requests com Retry

```python
import requests
from time import sleep
from typing import Dict, Any

def api_request_with_retry(url: str, headers: Dict[str, str] = None,
                          max_retries: int = 3, timeout: int = 30) -> Dict[Any, Any]:
    """Faz requisiÃ§Ã£o HTTP com retry automÃ¡tico"""

    for attempt in range(max_retries):
        try:
            response = requests.get(url, headers=headers, timeout=timeout)
            response.raise_for_status()
            return response.json()

        except requests.exceptions.RequestException as e:
            if attempt == max_retries - 1:  # Ãšltimo attempt
                raise Exception(f"Failed after {max_retries} attempts: {e}")

            wait_time = 2 ** attempt  # Backoff exponencial
            print(f"Attempt {attempt + 1} failed. Retrying in {wait_time}s...")
            sleep(wait_time)

# Uso
pokemon_data = api_request_with_retry(
    url="https://pokeapi.co/api/v2/pokemon/1",
    headers={"User-Agent": "Databricks-ETL/1.0"}
)
```

#### Multiprocessing para APIs

```python
from multiprocessing import Pool
from functools import partial
import json

def fetch_pokemon_data(pokemon_id: int, base_url: str) -> dict:
    """Busca dados de um Pokemon especÃ­fico"""
    url = f"{base_url}/pokemon/{pokemon_id}"
    data = api_request_with_retry(url)

    return {
        'pokemon_id': pokemon_id,
        'data': data,
        'extracted_at': datetime.utcnow().isoformat()
    }

def parallel_pokemon_extraction(pokemon_ids: list, base_url: str, workers: int = 4):
    """ExtraÃ§Ã£o paralela de dados de Pokemon"""

    fetch_func = partial(fetch_pokemon_data, base_url=base_url)

    with Pool(processes=workers) as pool:
        results = list(tqdm(
            pool.imap_unordered(fetch_func, pokemon_ids),
            total=len(pokemon_ids),
            desc="Extracting Pokemon data"
        ))

    return results

# Uso
pokemon_ids = range(1, 151)  # Primeira geraÃ§Ã£o
results = parallel_pokemon_extraction(pokemon_ids, "https://pokeapi.co/api/v2")
```

---

## ğŸ“š GlossÃ¡rio

### ğŸ”¤ Termos TÃ©cnicos

| Termo                  | DefiniÃ§Ã£o                                                                  | Exemplo/Contexto              |
| ---------------------- | -------------------------------------------------------------------------- | ----------------------------- |
| **ACID**               | Atomicity, Consistency, Isolation, Durability - propriedades de transaÃ§Ãµes | Delta Lake garante ACID       |
| **Auto Loader**        | Funcionalidade de ingestÃ£o incremental automÃ¡tica                          | `cloudFiles` format           |
| **Autoscaling**        | Ajuste automÃ¡tico de recursos baseado na demanda                           | Min/Max workers               |
| **Checkpoint**         | Ponto de controle para streams, garante exactly-once                       | `/path/_checkpoints/`         |
| **Cluster**            | Conjunto de VMs para processamento distribuÃ­do                             | Driver + Workers              |
| **Delta Lake**         | Formato de storage transacional sobre Parquet                              | `.delta` tables               |
| **Driver**             | NÃ³ coordenador do cluster Spark                                            | Orquestra Workers             |
| **External Location**  | ConfiguraÃ§Ã£o UC para acessar storage externo                               | S3/ADLS/GCS paths             |
| **Idempotente**        | OperaÃ§Ã£o que pode ser repetida sem efeitos colaterais                      | MERGE operations              |
| **Job Cluster**        | Cluster efÃªmero criado apenas para execuÃ§Ã£o de job                         | Cost-effective                |
| **Lakehouse**          | Arquitetura que combina data lake + warehouse                              | Delta + UC + Compute          |
| **Lineage**            | Rastreamento de origem e transformaÃ§Ãµes dos dados                          | Upstream/Downstream           |
| **LTS**                | Long Term Support - versÃ£o com suporte estendido                           | Runtime LTS                   |
| **Medallion**          | Arquitetura Bronze/Silver/Gold para data quality                           | raw â†’ bronze â†’ silver â†’ gold  |
| **Metastore**          | CatÃ¡logo central de metadados                                              | Unity Catalog component       |
| **Photon**             | Motor de execuÃ§Ã£o vetorizado da Databricks                                 | SQL/DataFrame acceleration    |
| **Runtime**            | VersÃ£o do Spark com otimizaÃ§Ãµes Databricks                                 | 13.3.x-scala2.12              |
| **Schema Evolution**   | Capacidade de evoluir schema automaticamente                               | Add/rename columns            |
| **Spot Instance**      | VM com preÃ§o reduzido mas preemptÃ­vel                                      | Cost optimization             |
| **Storage Credential** | Credenciais para acessar storage externo                                   | IAM roles, Service principals |
| **Time Travel**        | Consulta a versÃµes histÃ³ricas de tabelas Delta                             | VERSION AS OF                 |
| **Unity Catalog**      | Sistema de governanÃ§a e catalog da Databricks                              | Permissions, lineage          |
| **VACUUM**             | OperaÃ§Ã£o de limpeza de arquivos antigos em Delta                           | Garbage collection            |
| **Volume**             | Ãrea de arquivos gerenciada pelo Unity Catalog                             | `/Volumes/cat/schema/vol`     |
| **Widget**             | ParÃ¢metro de entrada em notebooks                                          | `dbutils.widgets`             |
| **Worker**             | NÃ³ de processamento no cluster Spark                                       | Executes tasks                |
| **Workflow**           | OrquestraÃ§Ã£o de tarefas (DAG)                                              | Databricks Jobs               |
| **Z-Order**            | TÃ©cnica de clustering para otimizar queries                                | `ZORDER BY (col)`             |

### ğŸ—ï¸ PadrÃµes de Arquitetura

| PadrÃ£o                     | DescriÃ§Ã£o                                         | BenefÃ­cios             |
| -------------------------- | ------------------------------------------------- | ---------------------- |
| **Medallion Architecture** | Bronze (raw) â†’ Silver (clean) â†’ Gold (business)   | Qualidade progressiva  |
| **Lambda Architecture**    | Batch + Stream processing paralelos               | Real-time + historical |
| **Kappa Architecture**     | Apenas stream processing                          | Simplicidade           |
| **Data Mesh**              | DomÃ­nios descentralizados com governanÃ§a federada | Scalability            |
| **ELT vs ETL**             | Extract-Load-Transform vs Extract-Transform-Load  | Cloud-native approach  |

### ğŸ“Š Formatos e Protocolos

| Formato/Protocolo | Uso                   | CaracterÃ­sticas      |
| ----------------- | --------------------- | -------------------- |
| **Parquet**       | Storage columnar      | CompressÃ£o, schemas  |
| **Delta**         | Transactional storage | ACID, time travel    |
| **JSON**          | Semi-structured data  | Human readable       |
| **Avro**          | Schema evolution      | Backwards compatible |
| **JDBC/ODBC**     | Database connectivity | BI tools integration |
| **REST API**      | Web services          | HTTP-based           |

### ğŸ”§ Ferramentas e IntegraÃ§Ãµes

| Ferramenta         | Categoria              | IntegraÃ§Ã£o Databricks   |
| ------------------ | ---------------------- | ----------------------- |
| **Power BI**       | Business Intelligence  | SQL Warehouse connector |
| **Tableau**        | Data Visualization     | JDBC/ODBC               |
| **Apache Airflow** | Workflow Orchestration | REST API integration    |
| **dbt**            | Analytics Engineering  | Databricks adapter      |
| **MLflow**         | ML Lifecycle           | Native integration      |
| **Git**            | Version Control        | Repos integration       |
| **Terraform**      | Infrastructure as Code | Databricks provider     |

---

## ğŸ¯ Resumo Executivo

### ğŸš€ Quick Start Checklist

```text
ğŸ“‹ Para comeÃ§ar com Databricks + Lakehouse:

1. ğŸ—ï¸ Setup Inicial
   â”œâ”€â”€ âœ… Criar workspace Databricks
   â”œâ”€â”€ âœ… Configurar Unity Catalog
   â”œâ”€â”€ âœ… Conectar storage (S3/ADLS/GCS)
   â””â”€â”€ âœ… Criar primeiro cluster

2. ğŸ“Š Dados e GovernanÃ§a
   â”œâ”€â”€ âœ… Criar estrutura Medallion (Bronze/Silver/Gold)
   â”œâ”€â”€ âœ… Configurar volumes para raw data
   â”œâ”€â”€ âœ… Definir permissÃµes Unity Catalog
   â””â”€â”€ âœ… Implementar data quality checks

3. ğŸ”„ Pipeline ETL
   â”œâ”€â”€ âœ… IngestÃ£o raw (APIs, files)
   â”œâ”€â”€ âœ… TransformaÃ§Ãµes Bronze â†’ Silver â†’ Gold
   â”œâ”€â”€ âœ… OrquestraÃ§Ã£o com Workflows
   â””â”€â”€ âœ… Monitoring e alertas

4. ğŸ“ˆ Analytics e BI
   â”œâ”€â”€ âœ… SQL Warehouse para BI
   â”œâ”€â”€ âœ… Dashboards Databricks
   â”œâ”€â”€ âœ… ConexÃ£o Power BI/Tableau
   â””â”€â”€ âœ… ExposiÃ§Ã£o via REST APIs
```

### ğŸ’¡ Principais BenefÃ­cios

| BenefÃ­cio             | DescriÃ§Ã£o                                        | Impacto             |
| --------------------- | ------------------------------------------------ | ------------------- |
| **ğŸƒâ€â™‚ï¸ Time-to-Market** | Desenvolvimento mais rÃ¡pido sem gerenciar infra  | Weeks â†’ Days        |
| **ğŸ’° Custo**          | Pay-per-use, autoscaling, spot instances         | 30-50% economia     |
| **ğŸ”’ GovernanÃ§a**     | Unity Catalog centralizado, lineage automÃ¡tico   | Compliance          |
| **âš¡ Performance**    | Photon, Delta optimizations, intelligent caching | 2-5x faster         |
| **ğŸ”§ Produtividade**  | Notebooks colaborativos, Git integration         | Developer happiness |

### ğŸ¯ Casos de Uso Ideais

```text
âœ… Databricks Ã© ideal para:
â”œâ”€â”€ ğŸ“Š Analytics e BI em grande escala
â”œâ”€â”€ ğŸ¤– Machine Learning end-to-end
â”œâ”€â”€ ğŸŒŠ Streaming real-time + batch
â”œâ”€â”€ ğŸ—ï¸ Data engineering complexo
â”œâ”€â”€ ğŸ”„ ETL/ELT com governanÃ§a
â””â”€â”€ ğŸ¢ Multi-cloud data platforms

âš ï¸ Considere alternativas para:
â”œâ”€â”€ ğŸ’¾ OLTP transactional systems
â”œâ”€â”€ ğŸ“± AplicaÃ§Ãµes de baixa latÃªncia (<100ms)
â”œâ”€â”€ ğŸ  On-premises only requirements
â””â”€â”€ ğŸ’¸ Very small datasets (<1GB)
```

---

**ğŸ‰ ParabÃ©ns!** VocÃª agora tem um guia completo para implementar pipelines ETL robustas no Databricks usando a arquitetura Lakehouse.

> ğŸ’¡ **PrÃ³ximos Passos**:
>
> 1. Clone este repositÃ³rio
> 2. Configure seu ambiente seguindo o passo-a-passo
> 3. Adapte os exemplos para seus dados especÃ­ficos
> 4. Implemente monitoring e alertas
> 5. Scale conforme necessÃ¡rio

**Happy Data Engineering!** ğŸš€ğŸ“Š

---

_Ãšltima atualizaÃ§Ã£o: Janeiro 2024 | VersÃ£o: 2.0_
